#google_colab_notebook
%%capture
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install trl peft accelerate bitsandbytes datasets pandas scikit-learn gradio

from torch import __version__ as torch_version
from packaging.version import Version as V
xformers_version = "xformers==0.0.27" if V(torch_version) < V("2.4.0") else "xformers"
!pip install --no-deps {xformers_version}
====================================================================================================
import torch
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset, Dataset
from sklearn.model_selection import train_test_split
import pandas as pd
import gradio as gr
from IPython.display import HTML

#checking gpu setup
def check_gpu_setup():
"""Verify GPU availability and print setup information"""
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
print(f"GPU: {torch.cuda.get_device_name(0)}")
else:
print("Warning: No GPU detected! Training will be very slow.")

check_gpu_setup()
====================================================================================================
model_name = "unsloth/Llama-3.2-1B-Instruct-bnb-4bit"
dataset_name = "justmalhar/fluent-dev"
max_seq_length = 4096
output_dir = "fluent-ui-generator"
====================================================================================================
def load_model():
model, tokenizer = FastLanguageModel.from_pretrained(
model_name=model_name,
max_seq_length=max_seq_length,
dtype=torch.float16,
load_in_4bit=True,
device_map="auto",
)
print("âœ“ Model loaded successfully!")
return model, tokenizer

model, tokenizer = load_model()
====================================================================================================
from datasets import DatasetDict
from typing import Union

def format_fluent_dev_chat(dataset: Union[DatasetDict, dict]) -> dict:
"""
Formats the 'justmalhar/fluent-dev' dataset into LLaMA 3 chat-style format
compatible with Unsloth or Hugging Face fine-tuning.

Input:
dataset: HF DatasetDict or dict with 'train', 'validation' splits
Output:
dict with 'train', 'validation' keys, each containing mapped datasets with "text"
"""

def format_example(example):
return {
"text": f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>
Generate a {example.get('category', 'component')} component with:
- Tags: {example.get('tags', [])}
- Colors: {example.get('colors', [])}
- Description: {example.get('description', '').strip()}
{example.get('instruction', '').strip()}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
{example.get('code', '').strip()}<|eot_id|>"""
}

return {
"train": dataset["train"].map(format_example, remove_columns=dataset["train"].column_names),
"validation": dataset["validation"].map(format_example, remove_columns=dataset["validation"].column_names)
}
====================================================================================================
from datasets import load_dataset

# Load dataset
raw_dataset = load_dataset("justmalhar/fluent-dev")

# format for dataset
formatted_dataset = format_fluent_dev_chat(raw_dataset)

# Step 3: Inspect
print(formatted_dataset["train"][0]["text"])
====================================================================================================

import torch
import transformers
import unsloth
import trl
import peft
import accelerate
import bitsandbytes
import datasets
import pandas
import sklearn
import gradio

print("All core libraries successfully imported!\n")
print(f"torch version : {torch.__version__}")
print(f"transformers version : {transformers.__version__}")
print(f"unsloth version : {unsloth.__version__}")
print(f"trl version : {trl.__version__}")
print(f"peft version : {peft.__version__}")
print(f"accelerate version : {accelerate.__version__}")
print(f"bitsandbytes version : {bitsandbytes.__version__}")
print(f"datasets version : {datasets.__version__}")
print(f"pandas version : {pandas.__version__}")
print(f"scikit-learn version : {sklearn.__version__}")
print(f"gradio version : {gradio.__version__}")

print(f"\nCUDA available : {torch.cuda.is_available()}")
if torch.cuda.is_available():
print(f"GPU Device : {torch.cuda.get_device_name(0)}")
====================================================================================================


# Utility to clean model response
def clean_response(decoded):
for tok in ["<|start_header_id|>", "<|end_header_id|>", "<|eot_id|>", "<|eos|>"]:
decoded = decoded.replace(tok, "")
decoded = decoded.strip()

# Remove any lines that repeat prompt headers
lines = decoded.splitlines()
cleaned = [line for line in lines if not line.strip().lower().startswith("generate a")]
return "\n".join(cleaned)


# Generate UI component
def generate_component(prompt):
# Build full chat prompt
full_prompt = f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n{prompt.strip()}<|eot_id|><|start_header_id|>assistant<|end_header_id|>"""

# Tokenize input
input_ids = tokenizer(full_prompt, return_tensors="pt").input_ids.to(model.device)

# Generate output
outputs = model.generate(
input_ids=input_ids,
max_new_tokens=512,
eos_token_id=tokenizer.convert_tokens_to_ids("<|eot_id|>"),
do_sample=True,
temperature=0.7,
top_p=0.9,
)

# Decode
decoded = tokenizer.decode(outputs[0], skip_special_tokens=False)

# Extract assistant's reply
if "<|start_header_id|>assistant<|end_header_id|>" in decoded:
decoded = decoded.split("<|start_header_id|>assistant<|end_header_id|>")[1]
if "<|eot_id|>" in decoded:
decoded = decoded.split("<|eot_id|>")[0]

return clean_response(decoded)
====================================================================================================

gr.Interface(
fn=generate_component,
inputs=gr.Textbox(lines=6, label="Describe your component"),
outputs=gr.Code(label="Generated JSX"),
title="ðŸ§  UI Component Generator (LLaMA 3)"
).launch(share=True)
